paper_id,title,abstract,authors,journal,year,volume,issue,pages,publisher,doi,url,type
paper_001,"Little ai, Big AI—Good AI, Bad AI",,Jim Euchner,Research-Technology Management,2019,62,3,10-12,Informa UK Limited,10.1080/08956308.2019.1587280,https://doi.org/10.1080/08956308.2019.1587280,journal-article
paper_002,Ai! AI...,,Kees Bals,PodoPost,2024,37,4,3-3,Springer Science and Business Media LLC,10.1007/s12480-024-2612-0,https://doi.org/10.1007/s12480-024-2612-0,journal-article
paper_003,AI vs. AI: Can AI Detect AI-Generated Images?,"The proliferation of Artificial Intelligence (AI) models such as Generative Adversarial Networks (GANs) has shown impressive success in image synthesis. Artificial GAN-based synthesized images have been widely spread over the Internet with the advancement in generating naturalistic and photo-realistic images. This might have the ability to improve content and media; however, it also constitutes a threat with regard to legitimacy, authenticity, and security. Moreover, implementing an automated system that is able to detect and recognize GAN-generated images is significant for image synthesis models as an evaluation tool, regardless of the input modality. To this end, we propose a framework for reliably detecting AI-generated images from real ones through Convolutional Neural Networks (CNNs). First, GAN-generated images were collected based on different tasks and different architectures to help with the generalization. Then, transfer learning was applied. Finally, several Class Activation Maps (CAM) were integrated to determine the discriminative regions that guided the classification model in its decision. Our approach achieved 100% on our dataset, i.e., Real or Synthetic Images (RSI), and a superior performance on other datasets and configurations in terms of its accuracy. Hence, it can be used as an evaluation tool in image generation. Our best detector was a pre-trained EfficientNetB4 fine-tuned on our dataset with a batch size of 64 and an initial learning rate of 0.001 for 20 epochs. Adam was used as an optimizer, and learning rate reduction along with data augmentation were incorporated.",Samah S. Baraheem; Tam V. Nguyen,Journal of Imaging,2023,9,10,199,MDPI AG,10.3390/jimaging9100199,https://doi.org/10.3390/jimaging9100199,journal-article
paper_004,When AI meets AI: analyzing AI bills using AI,"Abstract With the rapid advancement of Artificial Intelligence (AI) technology and its pervasive integration into society, governments worldwide have introduced a range of AI-related policies. In the United States, the use of AI technology has surged significantly since 2021, driven by the emergence of generative AI and its transformative potential. In response, the U.S. Congress has proposed numerous AI-related bills, reflecting growing legislative engagement with AI governance. This study examines 204 AI-related bills introduced during the 117th and 118th Congresses (2021–2024) through computational text analysis, employing topic modeling to identify recurring legislative themes and sentiment analysis to assess congressional attitudes toward AI policies. The findings reveal distinct variations in legislative focus and tone across chambers and political parties, offering a nuanced understanding of how AI-related issues are framed within U.S. policymaking. In addition, the results highlight how AI is connected to broader opportunities and concerns, including national security, technological innovation, and public service delivery. By applying machine learning techniques to legislative texts, this research provides a systematic and scalable approach to understanding AI policymaking. The study contributes to broader discussions on the partisan and institutional dynamics shaping AI legislation in the United States, offering insights into how emerging technologies are shaped by legislative priorities, regulatory attitudes, and broader political contexts.",Heonuk Ha,AI &amp; SOCIETY,2025,,,,Springer Science and Business Media LLC,10.1007/s00146-025-02466-9,https://doi.org/10.1007/s00146-025-02466-9,journal-article
paper_005,"AI, AI, Oh",,Ewen McColl,Dental Update,2025,52,6,369-370,Mark Allen Group,10.12968/denu.2025.52.6.369,https://doi.org/10.12968/denu.2025.52.6.369,journal-article
paper_006,AI Meets AI,Leveraging Query Executions to Improve Index Recommendations,Bailu Ding; Sudipto Das; Ryan Marcus; Wentao Wu; Surajit Chaudhuri; Vivek R. Narasayya,Proceedings of the 2019 International Conference on Management of Data,2019,,,1241-1258,ACM,10.1145/3299869.3324957,https://doi.org/10.1145/3299869.3324957,proceedings-article
paper_007,AI for AI: Using AI methods for classifying AI science documents,"Abstract Subject area classification is an important first phase in the entire process involved in bibliometrics. In this paper, we explore the possibility of using automated algorithms for classifying scientific papers related to Artificial Intelligence at the document level. The current process is semimanual and journal based, a realization that, we argue, opens up the potential for inaccuracies. To counter this, our proposed automated approach makes use of neural networks, specifically BERT. The classification accuracy of our model reaches 96.5%. In addition, the model was used for further classifying documents from 26 different subject areas from the Scopus database. Our findings indicate that a significant subset of existing Computer Science, Decision Science, and Mathematics publications could potentially be classified as AI-related. The same holds in particular cases in other science fields such as Medicine and Psychology or Arts and Humanities. The above indicate that in subject area classification processes, there is room for automatic approaches to be utilized in a complementary manner with traditional manual procedures.",Evi Sachini; Konstantinos Sioumalas-Christodoulou; Stefanos Christopoulos; Nikolaos Karampekios,Quantitative Science Studies,2022,3,4,1119-1132,MIT Press,10.1162/qss_a_00223,https://doi.org/10.1162/qss_a_00223,journal-article
paper_008,AI generations: from AI 1.0 to AI 4.0,"This paper proposes that Artificial Intelligence (AI) progresses through several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI), AI 3.0 (Physical AI), and a speculative AI 4.0 (Conscious AI). Each AI generation is driven by shifting priorities among algorithms, computing power, and data. AI 1.0 accompanied breakthroughs in pattern recognition and information processing, fueling advances in computer vision, natural language processing, and recommendation systems. AI 2.0 is built on these foundations through real-time decision-making in digital environments, leveraging reinforcement learning and adaptive planning for agentic AI applications. AI 3.0 extended intelligence into physical contexts, integrating robotics, autonomous vehicles, and sensor-fused control systems to act in uncertain real-world settings. Building on these developments, the proposed AI 4.0 puts forward the bold vision of self-directed AI capable of setting its own goals, orchestrating complex training regimens, and possibly exhibiting elements of machine consciousness. This paper traces the historical foundations of AI across roughly 70 years, mapping how changes in technological bottlenecks from algorithmic innovation to high-performance computing to specialized data have stimulated each generational leap. It further highlights the ongoing synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the ethical, regulatory, and philosophical challenges that arise when artificial systems approach (or aspire to) human-like autonomy. Ultimately, understanding these evolutions and their interdependencies is pivotal for guiding future research, crafting responsible governance, and ensuring that AI’s transformative potential benefits society.",Jiahao Wu; Hengxu You; Jing Du,Frontiers in Artificial Intelligence,2025,8,,,Frontiers Media SA,10.3389/frai.2025.1585629,https://doi.org/10.3389/frai.2025.1585629,journal-article
paper_009,The Genesis of AI by AI Integrated Circuit: Where AI Creates AI,"The typical Integrated Circuit (IC) development process commences with formulating specifications in natural language and subsequently proceeds to Register Transfer Level (RTL) implementation. RTL code is traditionally generated through manual efforts, using Hardware Description Languages (HDL) such as VHDL or Verilog. High-Level Synthesis (HLS), on the other hand, converts programming languages to HDL; these methods aim to streamline the engineering process, minimizing human effort and errors. Currently, Electronic Design Automation (EDA) algorithms have been improved with the use of AI, with new advancements in commercial (such as ChatGPT, Bard, among others) Large Language Models (LLM) and open-source tools presenting an opportunity to automate the chip design process. This paper centers on the creation of AI by AI, a Convolutional Neural Network (CNN) IC entirely developed by an LLM (ChatGPT-4), and its manufacturing with the first fabricable open-source Process Design Kit (PDK), SKY130A. The challenges, opportunities, advantages, disadvantages, conversation flow, and workflow involved in CNN IC development are presented in this work, culminating in the manufacturing process of AI by AI using a 130 nm technology, marking a groundbreaking achievement as possibly the world’s first CNN entirely written by AI for its IC manufacturing with a free PDK, being a benchmark for systems that can be generated today with LLMs.",Emilio Isaac Baungarten-Leon; Susana Ortega-Cisneros; Mohamed Abdelmoneum; Ruth Yadira Vidana Morales; German Pinedo-Diaz,Electronics,2024,13,9,1704,MDPI AG,10.3390/electronics13091704,https://doi.org/10.3390/electronics13091704,journal-article
paper_010,AI Programming,,Kazuhiro Motegi,Journal of The Japan Institute of Electronics Packaging,2024,27,3,243-248,Japan Institute of Electronics Packaging,10.5104/jiep.27.243,https://doi.org/10.5104/jiep.27.243,journal-article
